[base]
log_dir = log/

[TextCNN]
seed = 7874
split_size = 0.1
patience = 3
loss_type = ce_loss
num_labels = 10
dropout = 0.5
embed_dim = 300
filter_sizes = 3,4,5
num_filters = 256
vocab_size = 124713
learning_rate = 1e-3
num_epochs = 10
max_seq_len = 32
batch_size = 128
vocab_path = data/THUCNews/vocab.pkl
embedding_path = data/THUCNews/embedding.pkl
train_path = data/THUCNews/tokened/train.csv
dev_path = data/THUCNews/tokened/dev.csv
test_path = data/THUCNews/tokened/test.csv
model_path = output/model_data/best_model.pt
result_path = output/result/result.csv

[bert_fc]
epoch = 10